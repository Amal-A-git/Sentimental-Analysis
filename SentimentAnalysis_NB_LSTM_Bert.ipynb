{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6e933dkWj7R",
        "outputId": "31ecdb54-23c8-451a-b12f-30e2e8cdd505"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjccxKhXXb7-",
        "outputId": "7fa99c24-faa5-4ce3-afbc-dee97356c359"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download nltk data (only required the first time)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the dataset\n",
        "url =  '/content/drive/MyDrive/Mini_Projects/Sentiment_Analysis/Tweets.csv' # Use a different URL for demonstration\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(data.head())\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = text.lower()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text)\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "# Apply preprocessing\n",
        "data['text'] = data['text'].apply(preprocess_text)\n",
        "\n",
        "# Feature extraction\n",
        "X = data['text']\n",
        "y = data['airline_sentiment']\n",
        "\n",
        "# Convert labels to numerical values\n",
        "y = y.map({'negative': 0, 'neutral': 1, 'positive': 2})\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiiZaDBTYBwn",
        "outputId": "13315eaf-afc4-4c3d-c8c2-d43562aadb23"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
            "0  570306133677760513           neutral                        1.0000   \n",
            "1  570301130888122368          positive                        0.3486   \n",
            "2  570301083672813571           neutral                        0.6837   \n",
            "3  570301031407624196          negative                        1.0000   \n",
            "4  570300817074462722          negative                        1.0000   \n",
            "\n",
            "  negativereason  negativereason_confidence         airline  \\\n",
            "0            NaN                        NaN  Virgin America   \n",
            "1            NaN                     0.0000  Virgin America   \n",
            "2            NaN                        NaN  Virgin America   \n",
            "3     Bad Flight                     0.7033  Virgin America   \n",
            "4     Can't Tell                     1.0000  Virgin America   \n",
            "\n",
            "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
            "0                    NaN     cairdin                 NaN              0   \n",
            "1                    NaN    jnardino                 NaN              0   \n",
            "2                    NaN  yvonnalynn                 NaN              0   \n",
            "3                    NaN    jnardino                 NaN              0   \n",
            "4                    NaN    jnardino                 NaN              0   \n",
            "\n",
            "                                                text tweet_coord  \\\n",
            "0                @VirginAmerica What @dhepburn said.         NaN   \n",
            "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
            "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
            "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
            "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
            "\n",
            "               tweet_created tweet_location               user_timezone  \n",
            "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
            "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
            "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
            "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
            "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pipeline with a TfidfVectorizer and Naive Bayes classifier\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('clf', MultinomialNB())\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "print(f'Accuracy: {accuracy_score(y_test, y_pred):.4f}')\n",
        "print('Classification Report:')\n",
        "print(classification_report(y_test, y_pred, target_names=['negative', 'neutral', 'positive']))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjnnqsiJWlHU",
        "outputId": "de9badac-4509-4f77-8f3c-38d5ed3f68bf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6943\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.68      0.99      0.81      1889\n",
            "     neutral       0.73      0.12      0.21       580\n",
            "    positive       0.94      0.18      0.31       459\n",
            "\n",
            "    accuracy                           0.69      2928\n",
            "   macro avg       0.79      0.43      0.44      2928\n",
            "weighted avg       0.74      0.69      0.61      2928\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define some new data for predictions\n",
        "new_data = [\n",
        "    \"I love the customer service!\",\n",
        "    \"The flight was delayed, very frustrating.\",\n",
        "    \"It was okay, not great but not bad either.\"\n",
        "]\n",
        "\n",
        "# Preprocess and predict\n",
        "new_data_processed = [preprocess_text(text) for text in new_data]\n",
        "predictions = pipeline.predict(new_data_processed)\n",
        "\n",
        "# Map predictions back to sentiment labels\n",
        "sentiments = ['negative', 'neutral', 'positive']\n",
        "predicted_sentiments = [sentiments[pred] for pred in predictions]\n",
        "\n",
        "# Print predictions\n",
        "for text, sentiment in zip(new_data, predicted_sentiments):\n",
        "    print(f\"Text: {text}\\nPredicted Sentiment: {sentiment}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2ClStCPYR9N",
        "outputId": "fd0c816b-3ccf-4168-d058-4722b8b6b912"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: I love the customer service!\n",
            "Predicted Sentiment: negative\n",
            "\n",
            "Text: The flight was delayed, very frustrating.\n",
            "Predicted Sentiment: negative\n",
            "\n",
            "Text: It was okay, not great but not bad either.\n",
            "Predicted Sentiment: negative\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing\n",
        "data['text'] = data['text'].apply(preprocess_text)\n",
        "\n",
        "# Feature and label preparation\n",
        "X = data['text']\n",
        "y = data['airline_sentiment']\n",
        "\n",
        "# Convert labels to numerical values\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "y_encoded = to_categorical(y_encoded)\n",
        "\n",
        "# Tokenization and padding\n",
        "max_words = 10000  # Maximum number of words to keep\n",
        "max_len = 100      # Maximum length of sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(X)\n",
        "X_sequences = tokenizer.texts_to_sequences(X)\n",
        "X_padded = pad_sequences(X_sequences, maxlen=max_len)\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_encoded, test_size=0.3, random_state=42)\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_len))\n",
        "model.add(LSTM(128, return_sequences=True))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(LSTM(64))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss:.4f}')\n",
        "print(f'Test Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Plot training and validation loss/accuracy\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 684
        },
        "id": "Oh-N6SFjY1TT",
        "outputId": "eb6613a8-04be-4f40-f060-c92e5a91bc1e",
        "collapsed": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 28ms/step - accuracy: 0.6323 - loss: 0.8754 - val_accuracy: 0.7623 - val_loss: 0.6012\n",
            "Epoch 2/10\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.7981 - loss: 0.5004 - val_accuracy: 0.7937 - val_loss: 0.5421\n",
            "Epoch 3/10\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.8823 - loss: 0.3380 - val_accuracy: 0.7780 - val_loss: 0.6302\n",
            "Epoch 4/10\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.9064 - loss: 0.2550 - val_accuracy: 0.7835 - val_loss: 0.6412\n",
            "Epoch 5/10\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.9294 - loss: 0.2011 - val_accuracy: 0.7725 - val_loss: 0.6940\n",
            "\u001b[1m138/138\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7933 - loss: 0.5409\n",
            "Test Loss: 0.5421\n",
            "Test Accuracy: 0.7937\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 2 Axes>"
            ]
            },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate classification report\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_test_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "print('Classification Report:')\n",
        "print(classification_report(y_test_classes, y_pred_classes, target_names=label_encoder.classes_))\n",
        "\n",
        "# Predictions on new data\n",
        "new_data = [\n",
        "    \"I love the customer service!\",\n",
        "    \"The flight was delayed, very frustrating.\",\n",
        "    \"It was okay, not great but not bad either.\"\n",
        "]\n",
        "\n",
        "# Preprocess and pad new data\n",
        "new_data_processed = [preprocess_text(text) for text in new_data]\n",
        "new_data_sequences = tokenizer.texts_to_sequences(new_data_processed)\n",
        "new_data_padded = pad_sequences(new_data_sequences, maxlen=max_len)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(new_data_padded)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Map predictions back to sentiment labels\n",
        "sentiments = label_encoder.classes_\n",
        "predicted_sentiments = [sentiments[pred] for pred in predicted_classes]\n",
        "\n",
        "# Print predictions\n",
        "for text, sentiment in zip(new_data, predicted_sentiments):\n",
        "    print(f\"Text: {text}\\nPredicted Sentiment: {sentiment}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zc7bfXQiZhxD",
        "outputId": "77c8b617-4aa6-47af-c1b6-808c7b7777c6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m138/138\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.84      0.91      0.87      2814\n",
            "     neutral       0.62      0.55      0.58       884\n",
            "    positive       0.79      0.62      0.70       694\n",
            "\n",
            "    accuracy                           0.79      4392\n",
            "   macro avg       0.75      0.69      0.72      4392\n",
            "weighted avg       0.79      0.79      0.79      4392\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "Text: I love the customer service!\n",
            "Predicted Sentiment: positive\n",
            "\n",
            "Text: The flight was delayed, very frustrating.\n",
            "Predicted Sentiment: negative\n",
            "\n",
            "Text: It was okay, not great but not bad either.\n",
            "Predicted Sentiment: positive\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
        "\n",
        "# Build the CNN model\n",
        "model_cnn = Sequential()\n",
        "model_cnn.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_len))\n",
        "model_cnn.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "model_cnn.add(MaxPooling1D(pool_size=2))\n",
        "model_cnn.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "model_cnn.add(MaxPooling1D(pool_size=2))\n",
        "model_cnn.add(GlobalMaxPooling1D())\n",
        "model_cnn.add(Dense(3, activation='softmax'))\n",
        "\n",
        "model_cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the CNN model\n",
        "history_cnn = model_cnn.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the CNN model\n",
        "loss_cnn, accuracy_cnn = model_cnn.evaluate(X_test, y_test)\n",
        "print(f'Test Loss (CNN): {loss_cnn:.4f}')\n",
        "print(f'Test Accuracy (CNN): {accuracy_cnn:.4f}')\n",
        "\n",
        "# Classification report\n",
        "y_pred_cnn = model_cnn.predict(X_test)\n",
        "y_pred_classes_cnn = np.argmax(y_pred_cnn, axis=1)\n",
        "y_test_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "print('Classification Report (CNN):')\n",
        "print(classification_report(y_test_classes, y_pred_classes_cnn, target_names=label_encoder.classes_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5nKGzGEaGEr",
        "outputId": "9ad79ee3-3b9c-4253-9b7d-b20aa2e7e2bb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m161/161\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 28ms/step - accuracy: 0.6379 - loss: 0.8170 - val_accuracy: 0.7666 - val_loss: 0.5876\n",
            "Epoch 2/10\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8123 - loss: 0.4628 - val_accuracy: 0.7880 - val_loss: 0.5478\n",
            "Epoch 3/10\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9124 - loss: 0.2529 - val_accuracy: 0.7794 - val_loss: 0.6378\n",
            "\u001b[1m138/138\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.7662 - loss: 0.5866\n",
            "Test Loss (CNN): 0.5876\n",
            "Test Accuracy (CNN): 0.7666\n",
            "\u001b[1m138/138\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "Classification Report (CNN):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.81      0.93      0.87      2814\n",
            "     neutral       0.57      0.39      0.46       884\n",
            "    positive       0.70      0.58      0.63       694\n",
            "\n",
            "    accuracy                           0.77      4392\n",
            "   macro avg       0.70      0.63      0.66      4392\n",
            "weighted avg       0.75      0.77      0.75      4392\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import GRU\n",
        "\n",
        "# Build the GRU model\n",
        "model_gru = Sequential()\n",
        "model_gru.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_len))\n",
        "model_gru.add(GRU(128, return_sequences=True))\n",
        "model_gru.add(Dropout(0.5))\n",
        "model_gru.add(GRU(64))\n",
        "model_gru.add(Dropout(0.5))\n",
        "model_gru.add(Dense(3, activation='softmax'))\n",
        "\n",
        "model_gru.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the GRU model\n",
        "history_gru = model_gru.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the GRU model\n",
        "loss_gru, accuracy_gru = model_gru.evaluate(X_test, y_test)\n",
        "print(f'Test Loss (GRU): {loss_gru:.4f}')\n",
        "print(f'Test Accuracy (GRU): {accuracy_gru:.4f}')\n",
        "\n",
        "# Classification report\n",
        "y_pred_gru = model_gru.predict(X_test)\n",
        "y_pred_classes_gru = np.argmax(y_pred_gru, axis=1)\n",
        "y_test_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "print('Classification Report (GRU):')\n",
        "print(classification_report(y_test_classes, y_pred_classes_gru, target_names=label_encoder.classes_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfxDeywvaUvC",
        "outputId": "8c58ecf7-a982-442b-ed96-39498690b8a9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m161/161\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.6433 - loss: 0.8324 - val_accuracy: 0.7878 - val_loss: 0.5521\n",
            "Epoch 2/10\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.8321 - loss: 0.4307 - val_accuracy: 0.7862 - val_loss: 0.5588\n",
            "Epoch 3/10\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8944 - loss: 0.3033 - val_accuracy: 0.7842 - val_loss: 0.6370\n",
            "\u001b[1m138/138\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7865 - loss: 0.5513\n",
            "Test Loss (GRU): 0.5521\n",
            "Test Accuracy (GRU): 0.7878\n",
            "\u001b[1m138/138\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Classification Report (GRU):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.83      0.92      0.87      2814\n",
            "     neutral       0.66      0.45      0.54       884\n",
            "    positive       0.71      0.67      0.69       694\n",
            "\n",
            "    accuracy                           0.79      4392\n",
            "   macro avg       0.73      0.68      0.70      4392\n",
            "weighted avg       0.78      0.79      0.78      4392\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(url)\n",
        "\n",
        "# Apply preprocessing if needed\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "data['text'] = data['text'].apply(preprocess_text)\n",
        "\n",
        "# Split the data\n",
        "X = data[['text']]\n",
        "y = data['airline_sentiment']\n",
        "\n",
        "# Convert labels to numerical values\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Create a DataFrame for train and test split\n",
        "data['label'] = y_encoded\n",
        "train_df, test_df = train_test_split(data, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "PH0-CU9yctr-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the pre-trained BERT tokenizer and model for sequence classification\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)  # Assuming 3 sentiment classes\n",
        "\n",
        "# Tokenize the data (train_df and test_df should already be prepared as shown before)\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'].tolist(), padding='max_length', truncation=True, max_length=128)\n",
        "\n",
        "train_encodings = tokenize_function(train_df)\n",
        "test_encodings = tokenize_function(test_df)\n",
        "\n",
        "# Convert to PyTorch datasets\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = CustomDataset(train_encodings, train_df['label'].tolist())\n",
        "test_dataset = CustomDataset(test_encodings, test_df['label'].tolist())\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',              # output directory\n",
        "    num_train_epochs=10,                  # number of training epochs\n",
        "    per_device_train_batch_size=64,       # batch size for training\n",
        "    per_device_eval_batch_size=64,        # batch size for evaluation\n",
        "    warmup_steps=100,                    # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,                   # strength of weight decay\n",
        "    logging_dir='./logs',                # directory for storing logs\n",
        "    evaluation_strategy=\"epoch\",         # Evaluate every epoch\n",
        "    report_to=\"none\"  # Disables W&B logging entirely\n",
        ")\n",
        "\n",
        "# Define a custom function to compute the metrics\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    preds = np.argmax(predictions, axis=1)\n",
        "    report = classification_report(labels, preds, output_dict=True)\n",
        "    return {\n",
        "        'f1': report['weighted avg']['f1-score'],\n",
        "        'precision': report['weighted avg']['precision'],\n",
        "        'recall': report['weighted avg']['recall'],\n",
        "    }\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated ü§ó Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=test_dataset,           # evaluation dataset\n",
        "    compute_metrics=compute_metrics,     # the metrics function to compute during evaluation\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "eval_results = trainer.evaluate()\n",
        "print(\"Evaluation results:\", eval_results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "id": "IANx9vUEb4mV",
        "outputId": "32ba6afb-5faf-4c5a-d677-87778c67bd23"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1610' max='1610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1610/1610 38:56, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.502112</td>\n",
              "      <td>0.794511</td>\n",
              "      <td>0.816635</td>\n",
              "      <td>0.817395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.399857</td>\n",
              "      <td>0.848461</td>\n",
              "      <td>0.847309</td>\n",
              "      <td>0.851548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.456507</td>\n",
              "      <td>0.844300</td>\n",
              "      <td>0.842961</td>\n",
              "      <td>0.846767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.403600</td>\n",
              "      <td>0.620256</td>\n",
              "      <td>0.833582</td>\n",
              "      <td>0.840235</td>\n",
              "      <td>0.829918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.403600</td>\n",
              "      <td>0.704693</td>\n",
              "      <td>0.841575</td>\n",
              "      <td>0.841198</td>\n",
              "      <td>0.842213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.403600</td>\n",
              "      <td>0.797255</td>\n",
              "      <td>0.834610</td>\n",
              "      <td>0.833787</td>\n",
              "      <td>0.836293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.075300</td>\n",
              "      <td>0.834912</td>\n",
              "      <td>0.841167</td>\n",
              "      <td>0.840652</td>\n",
              "      <td>0.841758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.075300</td>\n",
              "      <td>0.909393</td>\n",
              "      <td>0.833317</td>\n",
              "      <td>0.831659</td>\n",
              "      <td>0.836521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.075300</td>\n",
              "      <td>0.947704</td>\n",
              "      <td>0.835706</td>\n",
              "      <td>0.834286</td>\n",
              "      <td>0.837887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.021700</td>\n",
              "      <td>0.960570</td>\n",
              "      <td>0.837277</td>\n",
              "      <td>0.836728</td>\n",
              "      <td>0.837887</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='69' max='69' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [69/69 00:30]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation results: {'eval_loss': 0.9605699777603149, 'eval_f1': 0.8372768532237875, 'eval_precision': 0.8367275291973034, 'eval_recall': 0.8378870673952641, 'eval_runtime': 31.1241, 'eval_samples_per_second': 141.112, 'eval_steps_per_second': 2.217, 'epoch': 10.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predictions on the test dataset\n",
        "predictions = trainer.predict(test_dataset)\n",
        "\n",
        "# The raw logits (output from the model before applying softmax)\n",
        "logits = predictions.predictions\n",
        "\n",
        "# Convert logits to predicted class labels\n",
        "predicted_labels = np.argmax(logits, axis=1)\n",
        "\n",
        "# Actual labels\n",
        "true_labels = test_df['label'].tolist()\n",
        "\n",
        "# Print some predictions and the corresponding actual labels for comparison\n",
        "for i in range(10):\n",
        "    print(f\"Text: {test_df['text'].iloc[i]}\")\n",
        "    print(f\"Predicted Label: {predicted_labels[i]}, True Label: {true_labels[i]}\")\n",
        "    print(\"------\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "O8C5MXyVu0QX",
        "outputId": "ca089269-1a29-4961-bdfb-1f6c7c234edb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: southwestair youre my early frontrunner for best airline oscars2016\n",
            "Predicted Label: 2, True Label: 2\n",
            "------\n",
            "Text: usairways how is it that my flt to ewr was cancelled flightled yet flts to nyc from usairways are still flying\n",
            "Predicted Label: 0, True Label: 0\n",
            "------\n",
            "Text: jetblue what is going on with your bdl to dca flights yesterday and today why is every single one getting delayed\n",
            "Predicted Label: 0, True Label: 0\n",
            "------\n",
            "Text: jetblue do they have to depart from washington dc\n",
            "Predicted Label: 1, True Label: 1\n",
            "------\n",
            "Text: jetblue i can probably find some of them are the ticket s on there\n",
            "Predicted Label: 1, True Label: 0\n",
            "------\n",
            "Text: united still waiting to hear back my wallet was stolen from one of your planes so would appreciate a resolution here\n",
            "Predicted Label: 0, True Label: 0\n",
            "------\n",
            "Text: united yes my flight was rebooked im just losing trust in you if i want to get anywhere on time\n",
            "Predicted Label: 0, True Label: 0\n",
            "------\n",
            "Text: jetblue thank you  what about paris  could we arrange something from there \n",
            "Predicted Label: 1, True Label: 1\n",
            "------\n",
            "Text: united not 100 sure however my ticket included one checked bag therefore this charge was extra and completely unanticipated\n",
            "Predicted Label: 0, True Label: 0\n",
            "------\n",
            "Text: usairways great crew for flight 504 phx to yvr tonight friendly efficient awesome job\n",
            "Predicted Label: 2, True Label: 2\n",
            "------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Calculate precision, recall, F1 score using classification_report\n",
        "report = classification_report(true_labels, predicted_labels, target_names=label_encoder.classes_)\n",
        "print(\"Classification Report:\\n\", report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBrhc-ExvcBG",
        "outputId": "e386db50-7ecd-4adc-ad5e-c9d1e9e53ea7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8379\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.90      0.91      0.90      2814\n",
            "     neutral       0.68      0.66      0.67       884\n",
            "    positive       0.78      0.78      0.78       694\n",
            "\n",
            "    accuracy                           0.84      4392\n",
            "   macro avg       0.79      0.78      0.78      4392\n",
            "weighted avg       0.84      0.84      0.84      4392\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
